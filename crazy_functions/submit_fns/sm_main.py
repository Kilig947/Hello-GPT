# encoding: utf-8
# @Time   : 2024/7/19
# @Author : Spike
# @Descr   :
import os
import json

from common.func_box import replace_expected_text, long_name_processing
from common import gr_converter_html
from common.toolbox import update_ui, update_ui_lastest_msg, get_conf
from crazy_functions import crazy_utils
from crazy_functions.pdf_fns.breakdown_txt import breakdown_text_to_satisfy_token_limit
from request_llms import bridge_all
from crazy_functions.submit_fns.content_process import (
    find_index_inlist, json_args_return, input_retrieval_file,
    file_extraction_intype, content_clear_links, content_img_vision_analyze

)


# <---------------------------------------æ’ä»¶ç”¨äº†éƒ½è¯´å¥½æ–¹æ³•----------------------------------------->
def split_list_token_limit(data, get_num, max_num=500):
    header_index = find_index_inlist(data_list=data, search_terms=['æ“ä½œæ­¥éª¤', 'å‰ç½®æ¡ä»¶', 'é¢„æœŸç»“æœ'])
    header_data = data[header_index]
    max_num -= len(str(header_data))
    temp_list = []
    split_data = []
    for index in data[header_index + 1:]:
        if get_num(str(temp_list)) > max_num:
            temp_list.insert(0, header_data)
            split_data.append(json.dumps(temp_list, ensure_ascii=False))
            temp_list = []
        else:
            temp_list.append(index)
    return split_data


def split_content_limit(inputs: str, llm_kwargs, chatbot, history) -> list:
    """
    Args:
        inputs: éœ€è¦æå–æ‹†åˆ†çš„æé—®ä¿¡æ¯
        llm_kwargs: è°ƒä¼˜å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²è®°å½•
    Returns: [æ‹†åˆ†1ï¼Œ æ‹†åˆ†2]
    """
    model = llm_kwargs['llm_model']
    if model.find('&') != -1:  # åˆ¤æ–­æ˜¯å¦å¤šæ¨¡å‹ï¼Œå¦‚æœå¤šæ¨¡å‹ï¼Œé‚£ä¹ˆä½¿ç”¨tokensæœ€å°‘çš„è¿›è¡Œæ‹†åˆ†
        models = str(model).split('&')
        _tokens = []
        _num_func = {}
        for _model in models:
            num_s = bridge_all.model_info[_model]['max_token']
            _tokens.append(num_s)
            _num_func[num_s] = _model
        all_tokens = min(_tokens)
        get_token_num = bridge_all.model_info[_num_func[all_tokens]]['token_cnt']
    else:
        all_tokens = bridge_all.model_info[model]['max_token']
        get_token_num = bridge_all.model_info[model]['token_cnt']
    max_token = all_tokens / 2  # è€ƒè™‘åˆ°å¯¹è¯+å›ç­”ä¼šè¶…è¿‡tokens,æ‰€ä»¥/2
    segments = []
    gpt_latest_msg = chatbot[-1][1]
    if type(inputs) is list:
        if get_token_num(str(inputs)) > max_token:
            bro_say = gpt_latest_msg + f'\n\næäº¤æ•°æ®é¢„è®¡ä¼šè¶…å‡º`{all_tokens}' \
                                       f'token`é™åˆ¶, å°†æŒ‰ç…§æ¨¡å‹æœ€å¤§å¯æ¥æ”¶tokenæ‹†åˆ†ä¸ºå¤šçº¿ç¨‹è¿è¡Œ\n\n---\n\n'
            yield from update_ui_lastest_msg(bro_say, chatbot, history)
            segments.extend(split_list_token_limit(data=inputs, get_num=get_token_num, max_num=max_token))
        else:
            segments.append(json.dumps(inputs, ensure_ascii=False))
    else:
        inputs = inputs.split('\n---\n')
        for input_ in inputs:
            if get_token_num(input_) > max_token:
                bro_say = gpt_latest_msg + f'\n\n{gr_converter_html.html_tag_color(input_[0][:20])} å¯¹è¯æ•°æ®é¢„è®¡ä¼šè¶…å‡º`{all_tokens}' \
                                           f'token`é™åˆ¶, å°†æŒ‰ç…§æ¨¡å‹æœ€å¤§å¯æ¥æ”¶tokenæ‹†åˆ†ä¸ºå¤šçº¿ç¨‹è¿è¡Œ'
                yield from update_ui_lastest_msg(bro_say, chatbot, history)
                segments.extend(
                    breakdown_text_to_satisfy_token_limit(input_, max_token, llm_kwargs['llm_model']))
            else:
                segments.append(input_)
    yield from update_ui(chatbot, history)
    return segments


def input_output_processing(gpt_response_collection, llm_kwargs, plugin_kwargs, chatbot, history,
                            kwargs_prompt: str = False, knowledge_base: bool = False):
    """
    Args:
        gpt_response_collection:  å¤šçº¿ç¨‹GPTçš„è¿”å›ç»“æœoræ–‡ä»¶è¯»å–å¤„ç†åçš„ç»“æœ
        plugin_kwargs: å¯¹è¯ä½¿ç”¨çš„æ’ä»¶å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²å¯¹è¯
        llm_kwargs:  è°ƒä¼˜å‚æ•°
        kwargs_prompt: Promptåç§°, å¦‚æœä¸ºFalseï¼Œåˆ™ä¸æ·»åŠ æç¤ºè¯
        knowledge_base: æ˜¯å¦å¯ç”¨çŸ¥è¯†åº“
    Returns: ä¸‹æ¬¡ä½¿ç”¨ï¼Ÿ
        inputs_arrayï¼Œ inputs_show_user_array
    """
    inputs_array = []
    inputs_show_user_array = []
    prompt_cls, = json_args_return(plugin_kwargs, ['æç¤ºè¯åˆ†ç±»'])
    ipaddr = llm_kwargs['ipaddr']
    if kwargs_prompt:
        from common.db.repository import prompt_repository
        prompt = prompt_repository.query_prompt(kwargs_prompt, prompt_cls, ipaddr, quote_num=True)
        if prompt:
            prompt = prompt.value
        else:
            raise ValueError('æŒ‡å®šçš„æç¤ºè¯ä¸å­˜åœ¨')
    else:
        prompt = '{{{v}}}'
    for inputs, you_say in zip(gpt_response_collection[1::2], gpt_response_collection[0::2]):
        content_limit = yield from split_content_limit(inputs, llm_kwargs, chatbot, history)
        try:
            plugin_kwargs['ä¸Šé˜¶æ®µæ–‡ä»¶'] = you_say
            plugin_kwargs[you_say] = {}
            plugin_kwargs[you_say]['åŸæµ‹è¯•ç”¨ä¾‹æ•°æ®'] = [json.loads(limit)[1:] for limit in content_limit]
            plugin_kwargs[you_say]['åŸæµ‹è¯•ç”¨ä¾‹è¡¨å¤´'] = json.loads(content_limit[0])[0]
        except Exception as f:
            print(f'è¯»å–åŸæµ‹è¯•ç”¨ä¾‹æŠ¥é”™ {f}')
        for limit in content_limit:
            # æ‹¼æ¥å†…å®¹ä¸æç¤ºè¯
            plugin_prompt = replace_expected_text(prompt, content=limit, expect='{{{v}}}')
            inputs_array.append(plugin_prompt)
            inputs_show_user_array.append(you_say)
    yield from update_ui(chatbot, history)
    return inputs_array, inputs_show_user_array


def submit_no_use_ui_task(txt_proc, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt, *args):
    inputs_show_user = None  # ä¸é‡å¤å±•ç¤º
    gpt_say = yield from crazy_utils.request_gpt_model_in_new_thread_with_ui_alive(
        inputs=txt_proc, inputs_show_user=inputs_show_user,
        llm_kwargs=llm_kwargs, chatbot=chatbot, history=history,
        sys_prompt="", refresh_interval=0.1
    )
    gpt_response_collection = [txt_proc, gpt_say]
    history.extend(gpt_response_collection)


def submit_multithreaded_tasks(inputs_array, inputs_show_user_array, llm_kwargs, chatbot, history, plugin_kwargs):
    """
    Args: æäº¤å¤šçº¿ç¨‹ä»»åŠ¡
        inputs_array: éœ€è¦æäº¤ç»™gptçš„ä»»åŠ¡åˆ—è¡¨
        inputs_show_user_array: æ˜¾ç¤ºåœ¨useré¡µé¢ä¸Šä¿¡æ¯
        llm_kwargs: è°ƒä¼˜å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²å¯¹è¯
        plugin_kwargs: æ’ä»¶è°ƒä¼˜å‚æ•°
    Returns:  å°†å¯¹è¯ç»“æœè¿”å›[è¾“å…¥, è¾“å‡º]
    """
    apply_history, = json_args_return(plugin_kwargs, ['ä¸Šä¸‹æ–‡å…³è”'], True)
    if apply_history:
        history_array = [[history] for _ in range(len(inputs_array))]
    else:
        history_array = [[] for _ in range(len(inputs_array))]
    # æ˜¯å¦è¦å¤šçº¿ç¨‹å¤„ç†
    if len(inputs_array) == 1:
        inputs_show_user = None  # ä¸é‡å¤å±•ç¤º
        gpt_say = yield from crazy_utils.request_gpt_model_in_new_thread_with_ui_alive(
            inputs=inputs_array[0], inputs_show_user=inputs_show_user,
            llm_kwargs=llm_kwargs, chatbot=chatbot, history=history_array[0],
            sys_prompt="", refresh_interval=0.1
        )
        gpt_response_collection = [inputs_array[0], gpt_say]
        history.extend(gpt_response_collection)
    else:
        gpt_response_collection = yield from crazy_utils.request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency(
            inputs_array=inputs_array,
            inputs_show_user_array=inputs_show_user_array,
            llm_kwargs=llm_kwargs,
            chatbot=chatbot,
            history_array=history_array,
            sys_prompt_array=["" for _ in range(len(inputs_array))],
            # max_workers=5,  # OpenAIæ‰€å…è®¸çš„æœ€å¤§å¹¶è¡Œè¿‡è½½
        )
    if apply_history:
        history.extend(gpt_response_collection)
    return gpt_response_collection


def func_æ‹†åˆ†ä¸æé—®(file_limit, llm_kwargs, plugin_kwargs, chatbot, history, plugin_prompt, knowledge_base):
    many_llm = json_args_return(plugin_kwargs, ['å¤šæ¨¡å‹å¹¶è¡Œ'], )
    if many_llm[0]:
        llm_kwargs['llm_model'] = "&".join([i for i in many_llm[0].split('&') if i])
    split_content_limit = yield from input_output_processing(file_limit, llm_kwargs, plugin_kwargs,
                                                             chatbot, history, kwargs_prompt=plugin_prompt,
                                                             knowledge_base=knowledge_base)
    inputs_array, inputs_show_user_array = split_content_limit
    gpt_response_collection = yield from submit_multithreaded_tasks(inputs_array, inputs_show_user_array,
                                                                    llm_kwargs, chatbot, history,
                                                                    plugin_kwargs)
    return gpt_response_collection


def user_input_embedding_content(user_input, chatbot, history, llm_kwargs, plugin_kwargs, valid_types):
    embedding_content = []  # å¯¹è¯å†…å®¹
    yield from update_ui(chatbot=chatbot, history=history, msg='ğŸ•µğŸ»â€è¶…çº§ä¾¦æ¢ï¼Œæ­£åœ¨åŠæ¡ˆï½')
    if plugin_kwargs.get('embedding_content'):
        embedding_content = plugin_kwargs['embedding_content']
        plugin_kwargs['embedding_content'] = ''  # ç”¨äº†å³åˆ»ä¸¢å¼ƒ
    else:
        chatbot.append([user_input, ''])
        download_format = gr_converter_html.get_fold_panel()
        chatbot[-1][1] = download_format(title='æ£€æµ‹æäº¤æ˜¯å¦å­˜åœ¨éœ€è¦è§£æçš„æ–‡ä»¶æˆ–é“¾æ¥...', content='')
        yield from update_ui(chatbot=chatbot, history=history, msg='Reader loading...')
        fp_mapping, download_status = input_retrieval_file(user_input, llm_kwargs, valid_types)
        download_status.update(fp_mapping)
        if fp_mapping:
            chatbot[-1][1] = download_format(title='é“¾æ¥è§£æå®Œæˆ', content=download_status, status='Done')
        elif download_status.get('status'):
            chatbot[-1][1] = download_format(title='è§£æé“¾æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æŠ¥é”™', content=download_status.get('status'), status='Done')
        content_mapping = yield from file_extraction_intype(fp_mapping, chatbot, history, llm_kwargs, plugin_kwargs)
        for content_fp in content_mapping:  # ä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªå¯¹è¯
            file_content = content_mapping[content_fp]
            # å°†è§£æçš„æ•°æ®æäº¤åˆ°æ­£æ–‡
            input_handle = user_input.replace(fp_mapping[content_fp], str(file_content))
            # å°†å…¶ä»–æ–‡ä»¶é“¾æ¥æ¸…é™¤
            user_clear = content_clear_links(input_handle, fp_mapping, content_mapping)
            # è¯†åˆ«å›¾ç‰‡é“¾æ¥å†…å®¹
            complete_input = yield from content_img_vision_analyze(user_clear, chatbot, history,
                                                                   llm_kwargs, plugin_kwargs)
            embedding_content.extend([os.path.basename(content_fp), complete_input])
        if not content_mapping:
            if len(user_input) > 100:  # æ²¡æœ‰æ¢æµ‹åˆ°ä»»ä½•æ–‡ä»¶ï¼Œå¹¶ä¸”æäº¤å¤§äº50ä¸ªå­—ç¬¦ï¼Œé‚£ä¹ˆè¿è¡Œå¾€ä¸‹èµ°
                chatbot[-1][1] = download_format(title='æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•æ–‡ä»¶', content=download_status, status='Done')
                yield from update_ui(chatbot=chatbot, history=history, msg='æ²¡æœ‰æ¢æµ‹åˆ°æ–‡ä»¶')
                # è¯†åˆ«å›¾ç‰‡é“¾æ¥å†…å®¹
                complete_input = yield from content_img_vision_analyze(user_input, chatbot, history,
                                                                       llm_kwargs, plugin_kwargs)
                embedding_content.extend([long_name_processing(user_input), complete_input])
            else:
                devs_document = get_conf('devs_document')
                status = '\n\næ²¡æœ‰æ¢æµ‹åˆ°ä»»ä½•æ–‡ä»¶ï¼Œå¹¶ä¸”æäº¤å­—ç¬¦å°‘äº50ï¼Œæ— æ³•å®Œæˆåç»­ä»»åŠ¡' \
                         f'è¯·åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥éœ€è¦è§£æçš„äº‘æ–‡æ¡£é“¾æ¥æˆ–æœ¬åœ°æ–‡ä»¶åœ°å€ï¼Œå¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£åˆ™ç”¨æ¢è¡Œæˆ–ç©ºæ ¼éš”å¼€ï¼Œç„¶åå†ç‚¹å‡»å¯¹åº”çš„æ’ä»¶\n\n' \
                         f'æ’ä»¶æ”¯æŒè§£ææ–‡æ¡£ç±»å‹`{valid_types}`' \
                         f"æœ‰é—®é¢˜ï¼Ÿè¯·è”ç³»`@spike` or æŸ¥çœ‹å¼€å‘æ–‡æ¡£{devs_document}"
                if chatbot[-1][1] is None:
                    chatbot[-1][1] = status
                chatbot[-1][1] += status
                yield from update_ui(chatbot=chatbot, history=history, msg='æ²¡æœ‰æ¢æµ‹åˆ°æ•°æ®')
                return []
        kb_upload, = json_args_return(plugin_kwargs, ['è‡ªåŠ¨å½•å…¥çŸ¥è¯†åº“'])
        files_list = [i for i in content_mapping if os.path.exists(i)]
        if kb_upload and files_list:
            from common.knowledge_base import kb_doc_api
            kb_doc_api.upload_docs_simple(files=files_list, knowledge_base_name=kb_upload)
    return embedding_content